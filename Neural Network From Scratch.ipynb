{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f89bc1d-bca8-482b-b40e-3582b6b7ca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f48a2daf-2883-4ab1-94c3-2b6e462e7af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense_Layer:\n",
    "    def __init__(self,n_inputs,n_neurons):\n",
    "        self.weights=0.01*np.random.randn(n_inputs,n_neurons)\n",
    "        self.biases=np.zeros((1,n_neurons))\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        self.inputs=inputs\n",
    "        self.output=np.dot(inputs,self.weights)+self.biases\n",
    "\n",
    "    def backward(self,dvalues):\n",
    "        self.dweights=np.dot(self.inputs.T,dvalues)\n",
    "        self.dbiases=np.sum(dvalues,axis=0,keepdims=True)\n",
    "        self.dinputs=np.dot(dvalues,self.weights.T)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2879f89-7a95-4569-a46a-5a18c8802e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Relu:\n",
    "    def forward(self,inputs):\n",
    "        self.inputs=inputs\n",
    "        self.output=np.maximum(0,inputs)\n",
    "   \n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs=dvalues.copy()\n",
    "        self.dinputs[self.inputs <=0] =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f05b8771-e91e-4a26-9dee-a7285028e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    def forward(self,inputs):\n",
    "        exp_values=np.exp(inputs- np.max(inputs , axis=1, keepdims=True))\n",
    "        probabilities=exp_values/np.sum(exp_values,axis=1,keepdims=True)\n",
    "        self.output=probabilities                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa5f3ce9-2a42-4866-93b6-672fbdb3f731",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def calculate(self,output,y):\n",
    "        sample_losses=self.forward(output,y)\n",
    "        data_loss=np.mean(sample_losses)\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecfb4842-888c-4f95-8a00-01c7feac9992",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Categorical_CrossEntropyLoss(Loss):\n",
    "    def forward(self,y_pred,y_true):\n",
    "        samples=len(y_pred)\n",
    "\n",
    "        y_pred_clipped=np.clip(y_pred,1e-7,1-1e-7)\n",
    "\n",
    "        if len(y_true.shape)==1:\n",
    "            correct_confidences= y_pred_clipped[\n",
    "            range(samples),\n",
    "            y_true\n",
    "            ]\n",
    "        elif len(y_true.shape)==2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        neg_log_likelihoods = -np.log(correct_confidences)\n",
    "        return neg_log_likelihoods\n",
    "    def backward(self,dvalues,y_true):\n",
    "        samples=len(dvalues)\n",
    "        labels=len(dvalues[0])\n",
    "\n",
    "        if len(y_true.shape)==1:\n",
    "            y_true=np.eye(labels)[y_true]\n",
    "\n",
    "        self.dinputs= -y_true/dvalues\n",
    "        self.dinputs= self.dinputs/samples      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6268f6dc-d1ee-4fb1-a2f1-2183e60dcf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_softmax_Loss_CategoricalCrossEntropy:\n",
    "    def __init__(self):\n",
    "        self.activation= Activation_Softmax()\n",
    "        self.loss= Categorical_CrossEntropyLoss()\n",
    "\n",
    "    def forward(self,inputs,y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.output=self.activation.output\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    def backward(self,dvalues,y_true):\n",
    "        samples=len(dvalues)\n",
    "        if len(y_true.shape)==2:\n",
    "            y_true = np.argmax(y_true,axis=1)\n",
    "        self.dinputs=dvalues.copy()\n",
    "        self.dinputs[range(samples),y_true]-=1\n",
    "        self.dinputs=self.dinputs/samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "421b98ff-0b77-4227-9dd1-5b2b40eb2d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Optimizer_Adam:\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "\n",
    "   \n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "\n",
    "\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b6ce928-7f1b-4832-affd-385222875ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(samples, classes):\n",
    "    X = np.zeros((samples*classes, 2))\n",
    "    y = np.zeros(samples*classes, dtype='uint8')\n",
    "    for class_number in range(classes):\n",
    "        ix = range(samples*class_number, samples*(class_number+1))\n",
    "        r = np.linspace(0.0, 1, samples)\n",
    "        t = np.linspace(class_number*4, (class_number+1)*4, samples) + np.random.randn(samples)*0.2\n",
    "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
    "        y[ix] = class_number\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03fb03d0-8c81-42ee-a5d5-1c45c2494a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=create_data(samples=100,classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec1a4b0a-2153-4221-b113-e839a547ae21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,acc:0.34,loss:1.0986060725028328\n",
      "epoch:100,acc:0.4533333333333333,loss:0.9634509101189161\n",
      "epoch:200,acc:0.57,loss:0.8500221335432286\n",
      "epoch:300,acc:0.72,loss:0.7111361775122444\n",
      "epoch:400,acc:0.7333333333333333,loss:0.6297734066323005\n",
      "epoch:500,acc:0.73,loss:0.5856513152101055\n",
      "epoch:600,acc:0.76,loss:0.5543730507220507\n",
      "epoch:700,acc:0.83,loss:0.47617879597691704\n",
      "epoch:800,acc:0.8133333333333334,loss:0.4322787922237544\n",
      "epoch:900,acc:0.8466666666666667,loss:0.4007855203577691\n",
      "epoch:1000,acc:0.8633333333333333,loss:0.37985788059096864\n",
      "epoch:1100,acc:0.8666666666666667,loss:0.35625692714709867\n",
      "epoch:1200,acc:0.87,loss:0.3418395183005995\n",
      "epoch:1300,acc:0.8666666666666667,loss:0.331655265071317\n",
      "epoch:1400,acc:0.8766666666666667,loss:0.32086391945722537\n",
      "epoch:1500,acc:0.87,loss:0.3133872771915697\n",
      "epoch:1600,acc:0.8733333333333333,loss:0.30576834564459887\n",
      "epoch:1700,acc:0.8533333333333334,loss:0.30574247030620727\n",
      "epoch:1800,acc:0.8766666666666667,loss:0.29560641195275117\n",
      "epoch:1900,acc:0.8733333333333333,loss:0.28952756075536284\n",
      "epoch:2000,acc:0.88,loss:0.28521311051275966\n",
      "epoch:2100,acc:0.87,loss:0.28308433156391144\n",
      "epoch:2200,acc:0.8833333333333333,loss:0.2779358726192769\n",
      "epoch:2300,acc:0.8833333333333333,loss:0.274633972281975\n",
      "epoch:2400,acc:0.8866666666666667,loss:0.27189662948948495\n",
      "epoch:2500,acc:0.8833333333333333,loss:0.2692909613516106\n",
      "epoch:2600,acc:0.8833333333333333,loss:0.26935693542420425\n",
      "epoch:2700,acc:0.8766666666666667,loss:0.26744751707959424\n",
      "epoch:2800,acc:0.8766666666666667,loss:0.26301188421258787\n",
      "epoch:2900,acc:0.87,loss:0.26708725458732496\n",
      "epoch:3000,acc:0.8833333333333333,loss:0.2620306200789429\n",
      "epoch:3100,acc:0.88,loss:0.25685049502516116\n",
      "epoch:3200,acc:0.8866666666666667,loss:0.25503044467173974\n",
      "epoch:3300,acc:0.8866666666666667,loss:0.2558195926671576\n",
      "epoch:3400,acc:0.8866666666666667,loss:0.2524484706888689\n",
      "epoch:3500,acc:0.89,loss:0.2504127518945186\n",
      "epoch:3600,acc:0.89,loss:0.24790127175961033\n",
      "epoch:3700,acc:0.88,loss:0.2465184880374838\n",
      "epoch:3800,acc:0.8833333333333333,loss:0.24650372956655087\n",
      "epoch:3900,acc:0.8866666666666667,loss:0.24401885921982416\n",
      "epoch:4000,acc:0.8866666666666667,loss:0.24258428212786456\n",
      "epoch:4100,acc:0.89,loss:0.24141801676662383\n",
      "epoch:4200,acc:0.8933333333333333,loss:0.24035011627039854\n",
      "epoch:4300,acc:0.89,loss:0.24305075425966682\n",
      "epoch:4400,acc:0.8866666666666667,loss:0.23917254147626732\n",
      "epoch:4500,acc:0.8866666666666667,loss:0.23752251646742764\n",
      "epoch:4600,acc:0.8866666666666667,loss:0.23682209900619267\n",
      "epoch:4700,acc:0.8866666666666667,loss:0.2356188036225193\n",
      "epoch:4800,acc:0.89,loss:0.2366529434763543\n",
      "epoch:4900,acc:0.8966666666666666,loss:0.23398633636303962\n",
      "epoch:5000,acc:0.8866666666666667,loss:0.23530826591954043\n",
      "epoch:5100,acc:0.8866666666666667,loss:0.2326012567191186\n",
      "epoch:5200,acc:0.8966666666666666,loss:0.2317741890760012\n",
      "epoch:5300,acc:0.89,loss:0.23288437855194843\n",
      "epoch:5400,acc:0.8866666666666667,loss:0.23098493117410637\n",
      "epoch:5500,acc:0.8933333333333333,loss:0.22986686500666537\n",
      "epoch:5600,acc:0.89,loss:0.22956571251652833\n",
      "epoch:5700,acc:0.89,loss:0.22884982091378295\n",
      "epoch:5800,acc:0.9,loss:0.2279872476813075\n",
      "epoch:5900,acc:0.8933333333333333,loss:0.22768711366352562\n",
      "epoch:6000,acc:0.89,loss:0.22751507295364737\n",
      "epoch:6100,acc:0.8933333333333333,loss:0.22662863853225138\n",
      "epoch:6200,acc:0.8933333333333333,loss:0.22612503814995155\n",
      "epoch:6300,acc:0.9,loss:0.225263762578606\n",
      "epoch:6400,acc:0.9,loss:0.22476281993044173\n",
      "epoch:6500,acc:0.8966666666666666,loss:0.2242513350709244\n",
      "epoch:6600,acc:0.89,loss:0.2291952480976417\n",
      "epoch:6700,acc:0.8933333333333333,loss:0.22366886964160695\n",
      "epoch:6800,acc:0.9,loss:0.22283319177578426\n",
      "epoch:6900,acc:0.9,loss:0.22269063340446313\n",
      "epoch:7000,acc:0.9,loss:0.22232295468840463\n",
      "epoch:7100,acc:0.8966666666666666,loss:0.22290574788855944\n",
      "epoch:7200,acc:0.9033333333333333,loss:0.2215240719789647\n",
      "epoch:7300,acc:0.9,loss:0.22080165232155097\n",
      "epoch:7400,acc:0.9,loss:0.2202897545205283\n",
      "epoch:7500,acc:0.8966666666666666,loss:0.21998912192882736\n",
      "epoch:7600,acc:0.9,loss:0.2197728548671536\n",
      "epoch:7700,acc:0.9,loss:0.21917915481295425\n",
      "epoch:7800,acc:0.8933333333333333,loss:0.21896553166498986\n",
      "epoch:7900,acc:0.8833333333333333,loss:0.2232241860411962\n",
      "epoch:8000,acc:0.9,loss:0.21812838637373616\n",
      "epoch:8100,acc:0.9,loss:0.21770813155658228\n",
      "epoch:8200,acc:0.9,loss:0.21742374244099205\n",
      "epoch:8300,acc:0.8966666666666666,loss:0.2185109707639927\n",
      "epoch:8400,acc:0.9,loss:0.21715154805963063\n",
      "epoch:8500,acc:0.9,loss:0.21650444235762886\n",
      "epoch:8600,acc:0.89,loss:0.21736007371817512\n",
      "epoch:8700,acc:0.8966666666666666,loss:0.21706304218940214\n",
      "epoch:8800,acc:0.9,loss:0.21569354552475384\n",
      "epoch:8900,acc:0.89,loss:0.21639420538013165\n",
      "epoch:9000,acc:0.9,loss:0.21535514894648883\n",
      "epoch:9100,acc:0.8833333333333333,loss:0.2185060178186447\n",
      "epoch:9200,acc:0.8933333333333333,loss:0.2150400296911779\n",
      "epoch:9300,acc:0.9,loss:0.21406697271300495\n",
      "epoch:9400,acc:0.9,loss:0.21373226131244527\n",
      "epoch:9500,acc:0.89,loss:0.22062166626243482\n",
      "epoch:9600,acc:0.8966666666666666,loss:0.21462898506702824\n",
      "epoch:9700,acc:0.8966666666666666,loss:0.22553064302738482\n",
      "epoch:9800,acc:0.8866666666666667,loss:0.21832639998865025\n",
      "epoch:9900,acc:0.9,loss:0.2124806721501871\n",
      "epoch:10000,acc:0.9033333333333333,loss:0.2160205224099632\n"
     ]
    }
   ],
   "source": [
    "dense1= Dense_Layer(2,64)\n",
    "activation_1=Activation_Relu()\n",
    "dense2=Dense_Layer(64,3)\n",
    "loss_activation=Activation_softmax_Loss_CategoricalCrossEntropy()\n",
    "optimizer=Optimizer_Adam(learning_rate=0.02,decay=1e-5)\n",
    "\n",
    "for epoch in range(10001):\n",
    "    dense1.forward(X)\n",
    "    activation_1.forward(dense1.output)\n",
    "    dense2.forward(activation_1.output)\n",
    "    loss=loss_activation.forward(dense2.output,y)\n",
    "\n",
    "    predictions=np.argmax(loss_activation.output,axis=1)\n",
    "    if len(y.shape)==2:\n",
    "        y=np.argmax(y,axis=1)\n",
    "    accuracy=np.mean(predictions==y)\n",
    "\n",
    "    if not epoch% 100:\n",
    "        print(f'epoch:{epoch},'+ f'acc:{accuracy},'+f'loss:{loss}')\n",
    "    loss_activation.backward(loss_activation.output ,y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation_1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation_1.dinputs)\n",
    "\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434bbb86-8cd4-46a2-9dfd-f3ef54030bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
